
PROBLEM STATEMENT :
CIFAR-10 is a dataset that consists of several images divided into the following 10 classes: 0. Airplanes
Cars
Birds
Cats
Deers
Dogs
Frogs
Horses
Ships
Trucks
Dataset consists of 60,000 32x32 color images, ie 6000 images per class.
We need to design a model to predict the label of these images correctly.
Data Source: https://www.cs.toronto.edu/~kriz/cifar.html
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import random
import pickle
import warnings
warnings.filterwarnings('ignore')
sns.set_style('dark')
Reading Data
from keras.datasets import cifar10
(X_train, y_train), (X_test, y_test) = cifar10.load_data()
Using TensorFlow backend.
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
(50000, 32, 32, 3)
(10000, 32, 32, 3)
(50000, 1)
(10000, 1)
Visualizing Data
data_dict = {0 : "Airplane",
             1 : "Car",
             2 : "Bird",
             3 : "Cat",
             4 : "Deer",
             5 : "Dog",
             6 : "Frog",
             7 : "Horse",
             8 : "Ship",
             9 : "Truck",
            }
def get_ten_random_images():
    """"
    function returns a dictionary of <label, idx> where idx is the random datapoint of class 'label'
    """"
    d = {}
    while len(d.keys()) < 10:
        idx = random.randint(0, len(y_train)-1)
        if y_train[idx][0] not in d.keys():
            d[y_train[idx][0]] = idx
    return d
 
d = get_ten_random_images()
rows= 2
cols=5
fig,axes = plt.subplots(rows, cols, figsize = (15,6))
axes = axes.ravel()
for i in range(1, rows*cols+1):
    axes[i-1].imshow(X_train[d[i-1]])
    axes[i-1].axis('off')
    axes[i-1].set_title(data_dict[i-1], fontsize=15)
fig.suptitle('10 Random Images from Dataset of each class', fontsize = 20)
plt.show()

 
Data Modeling
import keras
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
## Mean shifting and variance centering 
mean = np.mean(X_train, axis = (0,1,2,3))
std = np.std(X_train, axis = (0,1,2,3))
X_train = (X_train-mean)/(std + 1e-7)
X_test = (X_test-mean)/(std + 1e-7)
## Converting target variable into categorical matrix
y_train_cat = keras.utils.to_categorical(y_train,10)
y_test_cat = keras.utils.to_categorical(y_test,10)
input_shape = X_train.shape[1:]
input_shape
(32, 32, 3)
Model Training
from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dropout, BatchNormalization, Activation
from keras.optimizers import Adam
from keras.callbacks import TensorBoard
from keras import regularizers
 
weight_decay = 1e-4
model = Sequential()
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='same', input_shape = input_shape, kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(Conv2D(filters = 32, kernel_size = (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(2,2))
model.add(Dropout(0.2))

model.add(Conv2D(filters = 64, kernel_size = (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(Conv2D(filters = 64, kernel_size = (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(2,2))
model.add(Dropout(0.3))

model.add(Conv2D(filters = 128, kernel_size = (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(Conv2D(filters = 128, kernel_size = (3,3), padding='same', kernel_regularizer=regularizers.l2(weight_decay)))
model.add(Activation('elu'))
model.add(BatchNormalization())
model.add(MaxPooling2D(2,2))
model.add(Dropout(0.4))



model.add(Flatten())

model.add(Dense(units = 10, activation = 'softmax'))
    
model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_34 (Conv2D)           (None, 32, 32, 32)        896       
_________________________________________________________________
activation_19 (Activation)   (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_19 (Batc (None, 32, 32, 32)        128       
_________________________________________________________________
conv2d_35 (Conv2D)           (None, 32, 32, 32)        9248      
_________________________________________________________________
activation_20 (Activation)   (None, 32, 32, 32)        0         
_________________________________________________________________
batch_normalization_20 (Batc (None, 32, 32, 32)        128       
_________________________________________________________________
max_pooling2d_16 (MaxPooling (None, 16, 16, 32)        0         
_________________________________________________________________
dropout_16 (Dropout)         (None, 16, 16, 32)        0         
_________________________________________________________________
conv2d_36 (Conv2D)           (None, 16, 16, 64)        18496     
_________________________________________________________________
activation_21 (Activation)   (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_21 (Batc (None, 16, 16, 64)        256       
_________________________________________________________________
conv2d_37 (Conv2D)           (None, 16, 16, 64)        36928     
_________________________________________________________________
activation_22 (Activation)   (None, 16, 16, 64)        0         
_________________________________________________________________
batch_normalization_22 (Batc (None, 16, 16, 64)        256       
_________________________________________________________________
max_pooling2d_17 (MaxPooling (None, 8, 8, 64)          0         
_________________________________________________________________
dropout_17 (Dropout)         (None, 8, 8, 64)          0         
_________________________________________________________________
conv2d_38 (Conv2D)           (None, 8, 8, 128)         73856     
_________________________________________________________________
activation_23 (Activation)   (None, 8, 8, 128)         0         
_________________________________________________________________
batch_normalization_23 (Batc (None, 8, 8, 128)         512       
_________________________________________________________________
conv2d_39 (Conv2D)           (None, 8, 8, 128)         147584    
_________________________________________________________________
activation_24 (Activation)   (None, 8, 8, 128)         0         
_________________________________________________________________
batch_normalization_24 (Batc (None, 8, 8, 128)         512       
_________________________________________________________________
max_pooling2d_18 (MaxPooling (None, 4, 4, 128)         0         
_________________________________________________________________
dropout_18 (Dropout)         (None, 4, 4, 128)         0         
_________________________________________________________________
flatten_7 (Flatten)          (None, 2048)              0         
_________________________________________________________________
dense_13 (Dense)             (None, 10)                20490     
=================================================================
Total params: 309,290
Trainable params: 308,394
Non-trainable params: 896
_________________________________________________________________
model.compile(loss = 'categorical_crossentropy', optimizer= keras.optimizers.RMSprop(lr = 0.001, decay = 1e-6), metrics=['accuracy'])
epochs_hist = model.fit(X_train, y_train_cat, batch_size=64, epochs=50, validation_split=0.2, shuffle=True)
Train on 40000 samples, validate on 10000 samples
Epoch 1/50
40000/40000 [==============================] - 21s 513us/step - loss: 1.8407 - acc: 0.4510 - val_loss: 1.1795 - val_acc: 0.6137
Epoch 2/50
40000/40000 [==============================] - 18s 456us/step - loss: 1.1651 - acc: 0.6306 - val_loss: 1.0870 - val_acc: 0.6489
Epoch 3/50
40000/40000 [==============================] - 18s 455us/step - loss: 0.9728 - acc: 0.6906 - val_loss: 0.8171 - val_acc: 0.7433
Epoch 4/50
40000/40000 [==============================] - 18s 456us/step - loss: 0.8635 - acc: 0.7277 - val_loss: 0.8037 - val_acc: 0.7520
Epoch 5/50
40000/40000 [==============================] - 18s 458us/step - loss: 0.7894 - acc: 0.7587 - val_loss: 0.7848 - val_acc: 0.7647
Epoch 6/50
40000/40000 [==============================] - 18s 459us/step - loss: 0.7301 - acc: 0.7811 - val_loss: 0.7439 - val_acc: 0.7826
Epoch 7/50
40000/40000 [==============================] - 19s 486us/step - loss: 0.6857 - acc: 0.7971 - val_loss: 0.7531 - val_acc: 0.7812
Epoch 8/50
40000/40000 [==============================] - 19s 470us/step - loss: 0.6528 - acc: 0.8112 - val_loss: 0.7264 - val_acc: 0.7916
Epoch 9/50
40000/40000 [==============================] - 19s 475us/step - loss: 0.6184 - acc: 0.8247 - val_loss: 0.7802 - val_acc: 0.7862
Epoch 10/50
40000/40000 [==============================] - 19s 476us/step - loss: 0.5933 - acc: 0.8344 - val_loss: 0.6671 - val_acc: 0.8181
Epoch 11/50
40000/40000 [==============================] - 19s 471us/step - loss: 0.5736 - acc: 0.8420 - val_loss: 0.6773 - val_acc: 0.8194
Epoch 12/50
40000/40000 [==============================] - 19s 471us/step - loss: 0.5619 - acc: 0.8481 - val_loss: 0.6438 - val_acc: 0.8277
Epoch 13/50
40000/40000 [==============================] - 19s 471us/step - loss: 0.5371 - acc: 0.8592 - val_loss: 0.6816 - val_acc: 0.8171
Epoch 14/50
40000/40000 [==============================] - 19s 470us/step - loss: 0.5263 - acc: 0.8627 - val_loss: 0.6652 - val_acc: 0.8270
Epoch 15/50
40000/40000 [==============================] - 18s 456us/step - loss: 0.5106 - acc: 0.8696 - val_loss: 0.6575 - val_acc: 0.8318
Epoch 16/50
40000/40000 [==============================] - 18s 459us/step - loss: 0.5023 - acc: 0.8746 - val_loss: 0.6603 - val_acc: 0.8319
Epoch 17/50
40000/40000 [==============================] - 19s 465us/step - loss: 0.4929 - acc: 0.8765 - val_loss: 0.6589 - val_acc: 0.8341
Epoch 18/50
40000/40000 [==============================] - 19s 474us/step - loss: 0.4870 - acc: 0.8826 - val_loss: 0.6791 - val_acc: 0.8312
Epoch 19/50
40000/40000 [==============================] - 19s 480us/step - loss: 0.4794 - acc: 0.8871 - val_loss: 0.6707 - val_acc: 0.8382
Epoch 20/50
40000/40000 [==============================] - 19s 483us/step - loss: 0.4742 - acc: 0.8890 - val_loss: 0.6632 - val_acc: 0.8385
Epoch 21/50
40000/40000 [==============================] - 19s 485us/step - loss: 0.4688 - acc: 0.8913 - val_loss: 0.7386 - val_acc: 0.8279
Epoch 22/50
40000/40000 [==============================] - 19s 484us/step - loss: 0.4683 - acc: 0.8925 - val_loss: 0.7047 - val_acc: 0.8321
Epoch 23/50
40000/40000 [==============================] - 20s 490us/step - loss: 0.4592 - acc: 0.8961 - val_loss: 0.7083 - val_acc: 0.8353
Epoch 24/50
40000/40000 [==============================] - 20s 488us/step - loss: 0.4566 - acc: 0.8983 - val_loss: 0.7077 - val_acc: 0.8309
Epoch 25/50
40000/40000 [==============================] - 20s 489us/step - loss: 0.4479 - acc: 0.9017 - val_loss: 0.6973 - val_acc: 0.8340
Epoch 26/50
40000/40000 [==============================] - 19s 479us/step - loss: 0.4493 - acc: 0.9008 - val_loss: 0.7066 - val_acc: 0.8389
Epoch 27/50
40000/40000 [==============================] - 19s 485us/step - loss: 0.4439 - acc: 0.9054 - val_loss: 0.6921 - val_acc: 0.8433
Epoch 28/50
40000/40000 [==============================] - 20s 492us/step - loss: 0.4450 - acc: 0.9052 - val_loss: 0.7016 - val_acc: 0.8403oss: 0.4396
Epoch 29/50
40000/40000 [==============================] - 20s 489us/step - loss: 0.4379 - acc: 0.9082 - val_loss: 0.6983 - val_acc: 0.8437
Epoch 30/50
40000/40000 [==============================] - 20s 488us/step - loss: 0.4345 - acc: 0.9115 - val_loss: 0.6859 - val_acc: 0.8429
Epoch 31/50
40000/40000 [==============================] - 20s 488us/step - loss: 0.4374 - acc: 0.9095 - val_loss: 0.6981 - val_acc: 0.8424
Epoch 32/50
40000/40000 [==============================] - 20s 488us/step - loss: 0.4335 - acc: 0.9116 - val_loss: 0.6855 - val_acc: 0.8461
Epoch 33/50
40000/40000 [==============================] - 20s 490us/step - loss: 0.4294 - acc: 0.9141 - val_loss: 0.7040 - val_acc: 0.8397
Epoch 34/50
40000/40000 [==============================] - 19s 487us/step - loss: 0.4315 - acc: 0.9140 - val_loss: 0.7290 - val_acc: 0.8387
Epoch 35/50
40000/40000 [==============================] - 19s 471us/step - loss: 0.4279 - acc: 0.9153 - val_loss: 0.6866 - val_acc: 0.8463
Epoch 36/50
40000/40000 [==============================] - 19s 466us/step - loss: 0.4249 - acc: 0.9161 - val_loss: 0.7193 - val_acc: 0.8430
Epoch 37/50
40000/40000 [==============================] - 19s 466us/step - loss: 0.4257 - acc: 0.9160 - val_loss: 0.7048 - val_acc: 0.8447
Epoch 38/50
40000/40000 [==============================] - 19s 465us/step - loss: 0.4242 - acc: 0.9175 - val_loss: 0.7106 - val_acc: 0.8401
Epoch 39/50
40000/40000 [==============================] - 19s 465us/step - loss: 0.4247 - acc: 0.9181 - val_loss: 0.7084 - val_acc: 0.8435
Epoch 40/50
40000/40000 [==============================] - 19s 465us/step - loss: 0.4217 - acc: 0.9187 - val_loss: 0.7124 - val_acc: 0.8479
Epoch 41/50
40000/40000 [==============================] - 19s 465us/step - loss: 0.4205 - acc: 0.9197 - val_loss: 0.6814 - val_acc: 0.8489
Epoch 42/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4169 - acc: 0.9204 - val_loss: 0.6971 - val_acc: 0.8480
Epoch 43/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4191 - acc: 0.9189 - val_loss: 0.6979 - val_acc: 0.8473
Epoch 44/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4130 - acc: 0.9203 - val_loss: 0.7063 - val_acc: 0.8477
Epoch 45/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4104 - acc: 0.9221 - val_loss: 0.7024 - val_acc: 0.8491
Epoch 46/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4101 - acc: 0.9239 - val_loss: 0.7444 - val_acc: 0.8416
Epoch 47/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4117 - acc: 0.9245 - val_loss: 0.7182 - val_acc: 0.8469
Epoch 48/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4052 - acc: 0.9258 - val_loss: 0.7068 - val_acc: 0.8530
Epoch 49/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4050 - acc: 0.9251 - val_loss: 0.6826 - val_acc: 0.8567
Epoch 50/50
40000/40000 [==============================] - 19s 464us/step - loss: 0.4052 - acc: 0.9254 - val_loss: 0.7323 - val_acc: 0.8453
 
 
plt.figure(figsize=(15,5))
plt.subplot(1,2,1)
plt.plot(epochs_hist.history['val_acc'], label = 'CV Acc')
plt.plot(epochs_hist.history['acc'], label = 'Training Acc')
plt.title("Model Accuracy")
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid()
plt.legend()

plt.subplot(1,2,2)
plt.plot(epochs_hist.history['val_loss'], label = 'CV Loss')
plt.plot(epochs_hist.history['loss'], label = 'Training Loss')
plt.title("Model Loss")
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.legend()
plt.show()

Model Evaluation
## evaluating using the test set
score = model.evaluate(X_test, y_test_cat, batch_size=128, verbose=1)
10000/10000 [==============================] - 1s 146us/step
print("Accuracy on Test Set : {:.2f}%".format(score[1]*100))
Accuracy on Test Set : 84.21%
 
Notes:
With simple normalization, the cv accuracy tends to be less than 10%.
With Standardization (z-score), the cv accuracy increases to over 90%.
Next Step: Data augmentation to further increase the score.
